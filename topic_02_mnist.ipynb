{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffce6178-db6f-4e02-81af-110743e4a54a",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de86042-6532-46d8-9004-c0c5be6fa00d",
   "metadata": {},
   "source": [
    "## TOPIC 2: Introduction to Computer Vision. Intro to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b9c30-1d1a-4306-8ea4-109ec854bb4a",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0922224a-319e-4069-806d-fbf672643647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04dc8e-54f6-4a2e-bca4-d534d97498e9",
   "metadata": {},
   "source": [
    "### 2. MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041c46d-9b94-43ef-a14d-380e9de93f0d",
   "metadata": {},
   "source": [
    "About [MNIST dataset](https://yann.lecun.com/exdb/mnist/):\n",
    "- handwritten digits 0, 1, …, 9\n",
    "- 28x28 size (784 pixels)\n",
    "- training set of 60 000 and a test set of 10 000 examples\n",
    "- grey scale colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984f65a-43ba-42de-a46f-767186c82371",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d1a5fc-7020-46ec-9971-2eefececa26b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.52MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 199kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 688kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 61.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.MNIST('./data', download=True)\n",
    "test_data = torchvision.datasets.MNIST('data', train=False)\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7330a2a-a857-457e-af51-6dcfcf44292a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "image, label = train_data[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('label of image: {}'.format(label), fontsize=14)\n",
    "plt.subplot(1, 2, 2)\n",
    "image, label = train_data[1]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('label of image: {}'.format(label), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0f38e8-e912-4482-b856-5f4e51e7c383",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a91039-5b5e-4f07-85e0-e35f4523f60b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr = np.asarray(image)\n",
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88233a7-3a16-4f74-846b-00167a787ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60,\n",
       "        224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252,\n",
       "        252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253,\n",
       "        253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252,\n",
       "        179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,\n",
       "         84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,\n",
       "         28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,\n",
       "          0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,\n",
       "          0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85,\n",
       "        178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252,\n",
       "        252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252,\n",
       "        233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,\n",
       "         37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1db5b-525a-42ac-8277-be55a31d2411",
   "metadata": {},
   "source": [
    "### 2.2. Basic transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09a96e7-11a7-4db1-9e28-6b68a4054d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: NN likes input in range (0, 1)\n",
    "# Convert input images to tensors and normalize\n",
    "# Usually, 'transforms.ToTensor()' is used to turn\n",
    "# the input data in the range # of [0,255]\n",
    "# to a 3-dimensional Tensor.\n",
    "# This function automatically scales the\n",
    "# input data to the range of [0,1].\n",
    "# (This is equivalent to scaling the data down to 0,1)\n",
    "# The scaled mean and standard deviation\n",
    "# of the MNIST dataset (precalculated):\n",
    "# data_mean = 0.1307\n",
    "# data_std = 0.3081\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Normalize(  # apply if needed\n",
    "    #    (data_mean, ),\n",
    "    #    (data_std, )\n",
    "    #)\n",
    "    ])\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    'data',\n",
    "    train=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    'data',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ede4e-5976-4377-a741-7f5de7101945",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9568-5f11-4bb0-a019-13660eaeaec1",
   "metadata": {},
   "source": [
    "### 3.1. Data loaders for training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5197cd-50a1-46a3-8ad6-d5401c706082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "INPUT_SIZE = 28 * 28\n",
    "NUM_CLASSES = 10\n",
    "LEARNING_RATE = .001\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c0ecf7b-948b-4af0-b57d-d2cf0bd49583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0760b-c14f-481e-ba09-33a5246e9aba",
   "metadata": {},
   "source": [
    "### 3.2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751fe5e-ff83-4b01-9661-9263427c7dd0",
   "metadata": {},
   "source": [
    "Let's implement logistic regression with help of Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "206e4c80-6a99-4931-9803-fa2103c1abdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just one fully connected layer\n",
    "# will give us a regression\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        output = self.linear(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04040106-2593-47cb-9401-1eddfcfa1db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    INPUT_SIZE,\n",
    "    NUM_CLASSES\n",
    ")\n",
    "\n",
    "# The Cross-Entropy Loss is derived from the principles of maximum\n",
    "# likelihood estimation when applied to the task of classification.\n",
    "# Maximizing the likelihood is equivalent to minimizing the negative\n",
    "# log-likelihood. In classification, the likelihood function can be\n",
    "# expressed as the product of the probabilities of the correct classes:\n",
    "# Binary Cross-Entropy Loss and Multiclass Cross-Entropy Loss\n",
    "# are two variants of cross-entropy loss, each tailored to different\n",
    "# types of classification tasks:\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# about `Adam` optimizer\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "# it works well in most of cases\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48659166-ba11-4c41-8667-3662088dcacd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - validation loss: 0.514, validation accuracy: 90.57%        \n",
      "Epoch 1 - validation loss: 0.441, validation accuracy: 91.54%        \n",
      "Epoch 2 - validation loss: 0.415, validation accuracy: 91.94%        \n",
      "Epoch 3 - validation loss: 0.383, validation accuracy: 92.27%        \n",
      "Epoch 4 - validation loss: 0.391, validation accuracy: 92.43%        \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.autograd.Variable(images.view(-1, INPUT_SIZE))\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        # nullify gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = model(images)\n",
    "        # compute loss based on obtained value and actual label\n",
    "        compute_loss = loss(output, labels)\n",
    "        # backward propagation\n",
    "        compute_loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        correct += (predicted == labels).sum()\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                'Epoch {} - training [{}/{} ({:.0f}%)] loss: {:.3f}, accuracy: {:.2f}%'.format(\n",
    "                    epoch,\n",
    "                    i * len(images),\n",
    "                    len(train_loader.dataset),\n",
    "                    100 * i / len(train_loader),\n",
    "                    compute_loss.item(),\n",
    "                    float(correct * 100) / float(BATCH_SIZE * (i + 1))\n",
    "                ),\n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "    # check total accuracy of predicted value and actual label\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = torch.autograd.Variable(images.view(-1, INPUT_SIZE))\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        compute_loss = loss(output, labels)\n",
    "        # total labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Total correct predictions\n",
    "        accurate += (predicted == labels).sum()\n",
    "        accuracy_score = 100 * accurate/total\n",
    "\n",
    "    print('Epoch {} - validation loss: {:.3f}, validation accuracy: {:.2f}%        '.format(\n",
    "        epoch,\n",
    "        compute_loss.item(),\n",
    "        accuracy_score\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b66838-8a22-4f1f-ab55-bbf13680df51",
   "metadata": {},
   "source": [
    "### 3.3. CNN approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c60cd4-1a8a-4e9f-816d-0301d4d336b2",
   "metadata": {},
   "source": [
    "Now will use simple convolutional neural network with couple of convolutional layers. Few points before network building:\n",
    "- [about](https://docs.python.org/3/library/functions.html#super) `super` in Python. It allows you to call methods defined in the superclass from the subclass, enabling you to extend and customize the functionality inherited from the parent class\n",
    "- [about](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) `Conv2d` layer in Pytorch\n",
    "- [about](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) `Dropout` layers in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5496cfaa-b9a0-4dbf-82ef-c6815498ed39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding='valid')\n",
    "        self.dropout1 = nn.Dropout(.5)\n",
    "        self.dropout2 = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)  # you may need to comment this line for HA\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)  # you may need to comment this line for HA\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4324642f-92eb-4a53-9db9-c96b4bcf893e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN(NUM_CLASSES)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b8c52-1843-455a-9524-26ed9f09c964",
   "metadata": {},
   "source": [
    "### <font color='red'>HOME ASSIGNMENT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5ad67-9127-48ed-9dab-95a06c8c05c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "You have to make few experiments:\n",
    "1. Just run training CNN framework (code cell below) and observe if validation accuracy changes (get higher or lower)\n",
    "2. Try to play with `Dropout` layers e.g. remove them, re-run model creation and training cells of the notebook, observe if validation accuracy changes (get higher or lower)\n",
    "3. __(ADVANCED)__ Try CNN without `max_pool2d` layer, what has happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61a2c29-5fd2-4aaa-992b-27e87d6dfe9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - validation loss: 1.531, validation accuracy: 94.26%        \n",
      "Epoch 1 - validation loss: 1.529, validation accuracy: 95.89%        \n",
      "Epoch 2 - validation loss: 1.499, validation accuracy: 96.21%        \n",
      "Epoch 3 - validation loss: 1.483, validation accuracy: 96.64%        \n",
      "Epoch 4 - validation loss: 1.507, validation accuracy: 96.91%        \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.autograd.Variable(images)\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "\n",
    "        # Nullify gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = model(images)\n",
    "        # compute loss based on obtained value and actual label\n",
    "        compute_loss = loss(output, labels)\n",
    "        # backward propagation\n",
    "        compute_loss.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # total correct predictions\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        correct += (predicted == labels).sum()\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                'Epoch {} - training [{}/{} ({:.0f}%)] loss: {:.3f}, accuracy: {:.2f}%'.format(\n",
    "                    epoch,\n",
    "                    i * len(images),\n",
    "                    len(train_loader.dataset),\n",
    "                    100 * i / len(train_loader),\n",
    "                    compute_loss.item(),\n",
    "                    float(correct * 100) / float(BATCH_SIZE * (i + 1))\n",
    "                ),\n",
    "                end='\\r'\n",
    "            )\n",
    "\n",
    "    # check total accuracy of predicted value and actual label\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = torch.autograd.Variable(images)\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        compute_loss = loss(output, labels)\n",
    "        # total labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # total correct predictions\n",
    "        accurate += (predicted == labels).sum()\n",
    "        accuracy_score = 100 * accurate/total\n",
    "\n",
    "    print('Epoch {} - validation loss: {:.3f}, validation accuracy: {:.2f}%        '.format(\n",
    "        epoch,\n",
    "        compute_loss.item(),\n",
    "        accuracy_score\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94cd02-2090-44d0-885b-74561b11bbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
